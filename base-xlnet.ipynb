{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":7602085,"sourceType":"datasetVersion","datasetId":4425620}],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"from collections import defaultdict\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\n\nfrom transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\nfrom transformers import AdamW\n\nfrom torch.nn import BCEWithLogitsLoss\n\nimport nltk\nfrom nltk.stem import PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, hamming_loss","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Hyperparameters","metadata":{}},{"cell_type":"code","source":"MAX_LEN = 512\nTRAIN_BATCH_SIZE = 32\nVALID_BATCH_SIZE = 32\nTEST_BATCH_SIZE = 32\nEPOCHS = 10\nLEARNING_RATE = 1e-05\nTHRESHOLD = 0.5 # threshold for the sigmoid","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Dataset Class","metadata":{}},{"cell_type":"code","source":"tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n","metadata":{"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n\nclass CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, df, tokenizer, max_len, target_list):\n        self.tokenizer = tokenizer\n        self.df = df\n        self.title = list(df['File Contents'])\n        self.targets = self.df[target_list].values\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.title)\n\n    def __getitem__(self, index):\n        title = str(self.title[index])\n        title = \" \".join(title.split())\n        inputs = self.tokenizer.encode_plus(\n            title,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            padding='max_length',\n            return_token_type_ids=True,\n            truncation=True,\n            return_attention_mask=True,\n            return_tensors='pt'\n        )\n        return {\n            'input_ids': inputs['input_ids'].flatten(),\n            'attention_mask': inputs['attention_mask'].flatten(),\n            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n            'targets': torch.FloatTensor(self.targets[index]),\n            'title': title\n        }","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data","metadata":{}},{"cell_type":"code","source":"train_df = pd.read_csv('../input/ohsumed/train_csv.csv')\ntest_df = pd.read_csv('../input/ohsumed/test_csv.csv')\n# split test into test and validation datasets\ntrain_df, val_df = train_test_split(train_df, random_state=88, test_size=0.30, shuffle=True)\n\ntarget_list = list(train_df.columns[1:])\n\ntrain_dataset = CustomDataset(train_df, tokenizer, MAX_LEN, target_list)\nvalid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN, target_list)\ntest_dataset = CustomDataset(test_df, tokenizer, MAX_LEN, target_list)\n\nprint(train_dataset[0])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Data Loader","metadata":{}},{"cell_type":"code","source":"train_data_loader = torch.utils.data.DataLoader(train_dataset, \n    batch_size=TRAIN_BATCH_SIZE,\n    shuffle=True,\n    num_workers=0\n)\n\nval_data_loader = torch.utils.data.DataLoader(valid_dataset, \n    batch_size=VALID_BATCH_SIZE,\n    shuffle=False,\n    num_workers=0\n)\n\ntest_data_loader = torch.utils.data.DataLoader(test_dataset, \n    batch_size=TEST_BATCH_SIZE,\n    shuffle=False,\n    num_workers=0\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Device","metadata":{}},{"cell_type":"code","source":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## XLNet Model","metadata":{}},{"cell_type":"code","source":"class XLNETBase(nn.Module):\n    def __init__(self):\n        super(XLNETBase, self).__init__()\n        self.xlnet = XLNetModel.from_pretrained('xlnet-base-cased')\n        self.drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 23) # 23 is the number of classes in ohsumed dataset\n\n    def forward(self, input_ids, attention_mask, token_type_ids):\n        _, pooled_output = self.xlnet(\n            input_ids=input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids\n        )\n        output = self.drop(pooled_output)\n        return self.out(output)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = XLNETBase()\nmodel.to(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Loss and Optimizer","metadata":{}},{"cell_type":"code","source":"def loss_fn(outputs, targets):\n    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n\n# define the optimizer\noptimizer = AdamW(model.parameters(), lr = LEARNING_RATE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training the Model for One Epoch","metadata":{}},{"cell_type":"code","source":"def train_model(training_loader, model, optimizer):\n    losses = []\n    correct_predictions = 0\n    num_samples = 0\n    total_batches = len(training_loader)\n\n    # set model to training mode (activate dropout, batch norm)\n    model.train()\n\n    for batch_idx, data in enumerate(training_loader):\n        ids = data['input_ids'].to(device, dtype=torch.int)\n        mask = data['attention_mask'].to(device, dtype=torch.int)\n        token_type_ids = data['token_type_ids'].to(device, dtype=torch.int)\n        targets = data['targets'].to(device, dtype=torch.float)\n\n        # forward\n        outputs = model(ids, mask, token_type_ids)  # (batch,predict)=(32,8)\n        # print(outputs)\n        loss = loss_fn(outputs, targets)\n        # print(loss)\n        losses.append(loss.item())\n        # training accuracy, apply sigmoid, round (apply thresh 0.5)\n        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n        targets = targets.cpu().detach().numpy()\n        correct_predictions += np.sum(outputs == targets)\n        # print(correct_predictions)\n        num_samples += targets.size  # total number of elements in the 2D array\n\n        # backward\n        optimizer.zero_grad()\n        loss.backward()\n        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n        # grad descent step\n        optimizer.step()\n\n        # Print progress\n        # print(f\"Batch [{batch_idx+1}/{total_batches}], Loss: {loss.item()}\")\n\n    # returning: trained model, model accuracy, mean loss\n    return model, float(correct_predictions) / num_samples, np.mean(losses)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Validation","metadata":{}},{"cell_type":"code","source":"def eval_model(validation_loader, model):\n    model.eval()\n    final_targets = []\n    final_outputs = []\n    with torch.no_grad():\n        for data in validation_loader:\n            ids = data['input_ids'].to(device, dtype=torch.long)\n            mask = data['attention_mask'].to(device, dtype=torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n            targets = data['targets'].to(device, dtype=torch.float)\n            outputs = model(ids, mask, token_type_ids)\n            outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n            targets = targets.cpu().detach().numpy()\n            final_outputs.extend(outputs)\n            final_targets.extend(targets)\n    \n    final_outputs = np.array(final_outputs) >= THRESHOLD\n    # Calculating metrics\n    acc = accuracy_score(final_targets, final_outputs)\n    f1 = f1_score(final_targets, final_outputs, average='micro')  # Consider using 'macro' or 'weighted' based on your problem\n    precision = precision_score(final_targets, final_outputs, average='micro')\n    recall = recall_score(final_targets, final_outputs, average='micro')\n    hamming = hamming_loss(final_targets, final_outputs)\n    print(f\"Accuracy: {acc}\")\n    print(f\"F1 Score: {f1}\")\n    print(f\"Precision: {precision}\")\n    print(f\"Recall: {recall}\")\n    print(f\"Hamming Loss: {hamming}\")\n    # Detailed classification report\n    print(\"\\nClassification Report:\\n\", classification_report(final_targets, final_outputs, target_names=target_list))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training","metadata":{}},{"cell_type":"code","source":"history = defaultdict(list)\nbest_accuracy = 0\n\nfor epoch in range(1, EPOCHS+1):\n    print(f'Epoch {epoch}/{EPOCHS}')\n    model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n    eval_model(val_data_loader, model)\n\n    history['train_acc'].append(train_acc)\n    history['train_loss'].append(train_loss)\n    history['val_acc'].append(val_acc)\n    history['val_loss'].append(val_loss)\n    # save the best model\n    if val_acc > best_accuracy:\n        torch.save(model.state_dict(), \"XLNET_MLTC_model_state.bin\")\n        best_accuracy = val_acc","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Testing","metadata":{}},{"cell_type":"code","source":"# Loading pretrained model (best model)\nmodel = XLNETBase()\nmodel.load_state_dict(torch.load(\"XLNET_MLTC_model_state.bin\"))\nmodel = model.to(device)\n\n\n# Evaluate the model using the test data\neval_model(test_data_loader, model)","metadata":{},"execution_count":null,"outputs":[]}]}