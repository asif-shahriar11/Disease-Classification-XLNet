{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["from collections import defaultdict\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn as nn\n","\n","from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n","from transformers import AdamW\n","\n","from torch.nn import BCEWithLogitsLoss\n","\n","import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.corpus import stopwords\n","from nltk.tokenize import sent_tokenize, word_tokenize\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, hamming_loss"]},{"cell_type":"markdown","metadata":{},"source":["## Hyperparameters"]},{"cell_type":"code","execution_count":2,"metadata":{"trusted":true},"outputs":[],"source":["MAX_LEN = 512\n","TRAIN_BATCH_SIZE = 32\n","VALID_BATCH_SIZE = 32\n","TEST_BATCH_SIZE = 32\n","EPOCHS = 10\n","LEARNING_RATE = 1e-05\n","THRESHOLD = 0.5 # threshold for the sigmoid"]},{"cell_type":"markdown","metadata":{},"source":["## Dataset Class"]},{"cell_type":"code","execution_count":5,"metadata":{"_kg_hide-output":true,"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["spiece.model: 100%|██████████| 798k/798k [00:00<00:00, 1.75MB/s]\n","c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\User\\.cache\\huggingface\\hub\\models--xlnet-base-cased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n","To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n","  warnings.warn(message)\n","tokenizer.json: 100%|██████████| 1.38M/1.38M [00:01<00:00, 1.28MB/s]\n","config.json: 100%|██████████| 760/760 [00:00<?, ?B/s] \n"]}],"source":["tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased')\n"]},{"cell_type":"code","execution_count":7,"metadata":{"trusted":true},"outputs":[],"source":["class CustomDataset(torch.utils.data.Dataset):\n","    def __init__(self, df, tokenizer, max_len, target_list):\n","        self.tokenizer = tokenizer\n","        self.df = df\n","        self.title = list(df['File Contents'])\n","        self.targets = self.df[target_list].values\n","        self.max_len = max_len\n","\n","    def __len__(self):\n","        return len(self.title)\n","\n","    def __getitem__(self, index):\n","        title = str(self.title[index])\n","        title = \" \".join(title.split())\n","        inputs = self.tokenizer.encode_plus(\n","            title,\n","            None,\n","            add_special_tokens=True,\n","            max_length=self.max_len,\n","            padding='max_length',\n","            return_token_type_ids=True,\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","        )\n","        return {\n","            'input_ids': inputs['input_ids'].flatten(),\n","            'attention_mask': inputs['attention_mask'].flatten(),\n","            'token_type_ids': inputs[\"token_type_ids\"].flatten(),\n","            'targets': torch.FloatTensor(self.targets[index]),\n","            'title': title\n","        }"]},{"cell_type":"markdown","metadata":{},"source":["## Data"]},{"cell_type":"code","execution_count":9,"metadata":{"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["{'input_ids': tensor([    5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5,     5,     5,     5,     5,     5,\n","            5,     5,     5,     5,     5, 13663,   661,    13, 29362,    56,\n","         6748,    21,  1965, 11172, 12667,  1785,    20,  1242, 24366,    68,\n","        30583,   322,  4723,     9,   233,    25,   330,  1940,    13,   174,\n","        10745,  3512,  9757, 14781,    46,    23,     9,  4367,   620, 13741,\n","         7846,   330,  1940,    13,   174, 10745, 20168,    55, 20447,    33,\n","          914,  4012,   323,   150, 11518,    96,    13, 24366,    68,  7412,\n","        10174,  7663, 30583,    17,    10,  5108,   508,    11,   322,  4723,\n","            9,   233,     9,  3402, 18135,    25, 20168,    30,  2997,    33,\n","           18,    17,  7041,   661,  7623,    56,  6748,    17,    10, 25621,\n","           11,    31,   307,   156,    19,   198,    19,   425,    19,    21,\n","          241,    99,  7720,   508, 10624,     9,   240,   231,   191,    19,\n","           87, 13741,    55, 24008,    21,  5502,    55,  8410,    33,  1965,\n","        11172,  6336,    17,    10,   246, 14551,    11,    19, 21291,  1371,\n","        14094,    66,  6325,  2567,    17,    10,    96,  6333,    11,    19,\n","           21,  5129, 14095,    23,     9,  1275,   262,  6748,  7691,  1337,\n","         2395, 13760,    22,  3002,  5806,    23,    21,    79, 14551,  6520,\n","           24,  9708,   771,    25,  1242,  4216, 15510,   620,    25,    18,\n","        20168,     9,    32,  2849,    20, 11518,    96,    30,  3375,   199,\n","           18,    17, 20219,  8963,    31,   191,   156,    19,    57,    37,\n","          191,   198,    21, 24494,    36,  1144,    25, 13760, 16723,    17,\n","        23236,    21, 28386,  2139,   689,     9,    79,  1736, 18236,    30,\n","         1752,   161,  1242,  4216, 15510,   620,    25,    17, 20219,  8963,\n","           21,    81,    17,  1022, 10060,  6749,    25, 13760,  2662,     9,\n","            4,     3]), 'attention_mask': tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n","        1, 1, 1, 1, 1, 1, 1, 1]), 'token_type_ids': tensor([3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n","        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n","        0, 0, 0, 0, 0, 0, 0, 2]), 'targets': tensor([0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n","        0., 0., 0., 0., 0.]), 'title': 'Gamma-detecting probe and autoradiographic studies of radiolabeled antibody B72.3 in CX-1 colon xenografts. Nude mice bearing CX-1 colon tumors were injected with 50 microCi 125I-labeled monoclonal antibody (MAb) B72.3. Radioactivity in tumors was studied with the gamma detecting probe (GDP) on days 1, 3, 7, and 10 after MAb injection. On each day, two mice were sacrificed and sections were examined with autoradiography (ARG), immunoperoxidase methods (IMP), and routine stains. Mean probe counts showed increasing tumor to background ratios and ARG demonstrated a progressive increase in radionuclide in the tumors. The distribution of 125I was primarily around the vascular spaces on day 1, but by day 3 and progressively it appeared in tumor gland lumina and necrotic areas. A regional correlation was shown between radionuclide in vascular spaces and its sequestration in tumor elements.'}\n"]}],"source":["train_df = pd.read_csv('train_csv.csv')\n","test_df = pd.read_csv('test_csv.csv')\n","# split test into test and validation datasets\n","train_df, val_df = train_test_split(train_df, random_state=88, test_size=0.30, shuffle=True)\n","\n","target_list = list(train_df.columns[1:])\n","\n","train_dataset = CustomDataset(train_df, tokenizer, MAX_LEN, target_list)\n","valid_dataset = CustomDataset(val_df, tokenizer, MAX_LEN, target_list)\n","test_dataset = CustomDataset(test_df, tokenizer, MAX_LEN, target_list)\n","\n","print(train_dataset[0])"]},{"cell_type":"markdown","metadata":{},"source":["## Data Loader"]},{"cell_type":"code","execution_count":10,"metadata":{"trusted":true},"outputs":[],"source":["train_data_loader = torch.utils.data.DataLoader(train_dataset, \n","    batch_size=TRAIN_BATCH_SIZE,\n","    shuffle=True,\n","    num_workers=0\n",")\n","\n","val_data_loader = torch.utils.data.DataLoader(valid_dataset, \n","    batch_size=VALID_BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=0\n",")\n","\n","test_data_loader = torch.utils.data.DataLoader(test_dataset, \n","    batch_size=TEST_BATCH_SIZE,\n","    shuffle=False,\n","    num_workers=0\n",")"]},{"cell_type":"markdown","metadata":{},"source":["## Device"]},{"cell_type":"code","execution_count":11,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["device(type='cpu')"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","device"]},{"cell_type":"markdown","metadata":{},"source":["## XLNet Model"]},{"cell_type":"code","execution_count":18,"metadata":{"trusted":true},"outputs":[],"source":["class XLNETBase(nn.Module):\n","    def __init__(self):\n","        super(XLNETBase, self).__init__()\n","        self.xlnet = XLNetModel.from_pretrained('xlnet-base-cased')\n","        self.drop = nn.Dropout(0.3)\n","        self.out = nn.Linear(768, 23)  # Assuming 23 classes for classification\n","\n","    def forward(self, input_ids, attention_mask, token_type_ids):\n","        # Get the full output\n","        outputs = self.xlnet(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids)\n","\n","        # The last hidden state is the first element of the output\n","        last_hidden_state = outputs[0]\n","\n","        # Pooling operation if needed, for example, using the [CLS] token's embedding\n","        # Assuming [CLS] is the first token, similar to BERT. Adjust as needed.\n","        pooled_output = last_hidden_state[:, 0]\n","\n","        output = self.drop(pooled_output)\n","        return self.out(output)\n"]},{"cell_type":"code","execution_count":19,"metadata":{"trusted":true},"outputs":[{"data":{"text/plain":["XLNETBase(\n","  (xlnet): XLNetModel(\n","    (word_embedding): Embedding(32000, 768)\n","    (layer): ModuleList(\n","      (0-11): 12 x XLNetLayer(\n","        (rel_attn): XLNetRelativeAttention(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","        )\n","        (ff): XLNetFeedForward(\n","          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n","          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n","          (dropout): Dropout(p=0.1, inplace=False)\n","          (activation_function): GELUActivation()\n","        )\n","        (dropout): Dropout(p=0.1, inplace=False)\n","      )\n","    )\n","    (dropout): Dropout(p=0.1, inplace=False)\n","  )\n","  (drop): Dropout(p=0.3, inplace=False)\n","  (out): Linear(in_features=768, out_features=23, bias=True)\n",")"]},"execution_count":19,"metadata":{},"output_type":"execute_result"}],"source":["model = XLNETBase()\n","model.to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Loss and Optimizer"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["c:\\Users\\User\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\transformers\\optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]}],"source":["def loss_fn(outputs, targets):\n","    return torch.nn.BCEWithLogitsLoss()(outputs, targets)\n","\n","# define the optimizer\n","optimizer = AdamW(model.parameters(), lr = LEARNING_RATE)"]},{"cell_type":"markdown","metadata":{},"source":["## Training the Model for One Epoch"]},{"cell_type":"code","execution_count":20,"metadata":{},"outputs":[],"source":["def train_model(training_loader, model, optimizer):\n","    losses = []\n","    correct_predictions = 0\n","    num_samples = 0\n","    total_batches = len(training_loader)\n","\n","    # set model to training mode (activate dropout, batch norm)\n","    model.train()\n","\n","    for batch_idx, data in enumerate(training_loader):\n","        ids = data['input_ids'].to(device, dtype=torch.int)\n","        mask = data['attention_mask'].to(device, dtype=torch.int)\n","        token_type_ids = data['token_type_ids'].to(device, dtype=torch.int)\n","        targets = data['targets'].to(device, dtype=torch.float)\n","\n","        # forward\n","        outputs = model(ids, mask, token_type_ids)  # (batch,predict)=(32,8)\n","        # print(outputs)\n","        loss = loss_fn(outputs, targets)\n","        # print(loss)\n","        losses.append(loss.item())\n","        # training accuracy, apply sigmoid, round (apply thresh 0.5)\n","        outputs = torch.sigmoid(outputs).cpu().detach().numpy().round()\n","        targets = targets.cpu().detach().numpy()\n","        correct_predictions += np.sum(outputs == targets)\n","        # print(correct_predictions)\n","        num_samples += targets.size  # total number of elements in the 2D array\n","\n","        # backward\n","        optimizer.zero_grad()\n","        loss.backward()\n","        nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n","        # grad descent step\n","        optimizer.step()\n","\n","        # Print progress\n","        # print(f\"Batch [{batch_idx+1}/{total_batches}], Loss: {loss.item()}\")\n","\n","    # returning: trained model, model accuracy, mean loss\n","    return model, float(correct_predictions) / num_samples, np.mean(losses)"]},{"cell_type":"markdown","metadata":{},"source":["## Model Validation"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["def eval_model(validation_loader, model, loss_fn):\n","    model.eval()\n","    final_targets = []\n","    final_outputs = []\n","    losses = []\n","\n","    with torch.no_grad():\n","        for data in validation_loader:\n","            ids = data['input_ids'].to(device, dtype=torch.long)\n","            mask = data['attention_mask'].to(device, dtype=torch.long)\n","            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n","            targets = data['targets'].to(device, dtype=torch.float)\n","            \n","            outputs = model(ids, mask, token_type_ids)\n","            loss = loss_fn(outputs, targets)\n","            losses.append(loss.item())\n","\n","            outputs = torch.sigmoid(outputs).cpu().detach().numpy()\n","            targets = targets.cpu().detach().numpy()\n","            final_outputs.extend(outputs)\n","            final_targets.extend(targets)\n","    \n","    final_outputs = np.array(final_outputs) >= THRESHOLD\n","    # Calculating metrics\n","    acc = accuracy_score(final_targets, final_outputs)\n","    f1 = f1_score(final_targets, final_outputs, average='micro')  # Consider using 'macro' or 'weighted' based on your problem\n","    precision = precision_score(final_targets, final_outputs, average='micro')\n","    recall = recall_score(final_targets, final_outputs, average='micro')\n","    hamming = hamming_loss(final_targets, final_outputs)\n","    \n","    average_loss = np.mean(losses)\n","    \n","    print(f\"Accuracy: {acc}\")\n","    print(f\"F1 Score: {f1}\")\n","    print(f\"Precision: {precision}\")\n","    print(f\"Recall: {recall}\")\n","    print(f\"Hamming Loss: {hamming}\")\n","    print(f\"Average Loss: {average_loss}\")\n","    # Detailed classification report\n","    print(\"\\nClassification Report:\\n\", classification_report(final_targets, final_outputs, target_names=target_list))\n","\n","    return acc, average_loss\n","\n","\n"]},{"cell_type":"markdown","metadata":{},"source":["## Training"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Epoch 1/10\n"]}],"source":["history = defaultdict(list)\n","best_accuracy = 0\n","\n","for epoch in range(1, EPOCHS+1):\n","    print(f'Epoch {epoch}/{EPOCHS}')\n","    model, train_acc, train_loss = train_model(train_data_loader, model, optimizer)\n","    val_acc, val_loss = eval_model(val_data_loader, model)\n","\n","    history['train_acc'].append(train_acc)\n","    history['train_loss'].append(train_loss)\n","    history['val_acc'].append(val_acc)\n","    history['val_loss'].append(val_loss)\n","    # save the best model\n","    if val_acc > best_accuracy:\n","        torch.save(model.state_dict(), \"XLNET_MLTC_model_state.bin\")\n","        best_accuracy = val_acc"]},{"cell_type":"markdown","metadata":{},"source":["## Testing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Loading pretrained model (best model)\n","model = XLNETBase()\n","model.load_state_dict(torch.load(\"XLNET_MLTC_model_state.bin\"))\n","model = model.to(device)\n","\n","\n","# Evaluate the model using the test data\n","eval_model(test_data_loader, model)"]}],"metadata":{"kaggle":{"accelerator":"gpu","dataSources":[{"datasetId":4425620,"sourceId":7602085,"sourceType":"datasetVersion"}],"dockerImageVersionId":30648,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.10"}},"nbformat":4,"nbformat_minor":4}
